import asyncio
import logging
from playwright.async_api import async_playwright
from urllib.parse import urljoin
from app.db.database import SessionLocal
from app.db.models import Novela

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("CatalogScraper")

async def scrape_novel_catalog(catalog_url: str = "https://twkan.com/novels/hot", max_pages: int = 5):
    """
    Scrapea el cat√°logo de novelas de twkan.com y guarda las URLs en la base de datos.
    
    Args:
        catalog_url: URL del cat√°logo (por defecto: novelas populares)
        max_pages: N√∫mero m√°ximo de p√°ginas a scrapear
    """
    db = SessionLocal()
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=False,
            args=["--disable-blink-features=AutomationControlled"]
        )
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        await context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        page = await context.new_page()
        
        novelas_encontradas = []
        urls_ya_en_db = {n.fuente_scraping for n in db.query(Novela.fuente_scraping).all() if n.fuente_scraping}
        
        try:
            for page_num in range(1, max_pages + 1):
                # Construir URL de la p√°gina seg√∫n el patr√≥n de twkan.com
                # Formato: https://twkan.com/novels/newhot_0_0_1.html
                if page_num == 1:
                    url = catalog_url
                else:
                    # Extraer la categor√≠a de la URL base
                    # Ej: https://twkan.com/novels/hot ‚Üí hot
                    # Ej: https://twkan.com/novels/newhot_0_0_1.html ‚Üí newhot
                    if '_0_0_' in catalog_url:
                        # Ya tiene el formato completo, solo cambiar el n√∫mero
                        base = catalog_url.rsplit('_', 1)[0]  # Quita el √∫ltimo n√∫mero
                        url = f"{base}_{page_num}.html"
                    else:
                        # Formato simple, construir el patr√≥n completo
                        base = catalog_url.rstrip('/').replace('.html', '')
                        category = base.split('/')[-1]  # Extraer 'hot', 'new', etc.
                        url = f"https://twkan.com/novels/{category}_0_0_{page_num}.html"
                
                logger.info(f"\n{'='*80}")
                logger.info(f"üìñ Scrapeando p√°gina {page_num}/{max_pages}: {url}")
                logger.info(f"{'='*80}")
                
                try:
                    await page.goto(url, timeout=60000, wait_until="domcontentloaded")
                    await asyncio.sleep(2)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error al cargar p√°gina {page_num}: {e}")
                    continue
                
                # Buscar todos los enlaces a novelas
                # En twkan, los enlaces a novelas suelen estar en formato /book/XXXXX.html
                enlaces = await page.query_selector_all("a[href*='/book/']")
                
                logger.info(f"üîç Encontrados {len(enlaces)} enlaces en la p√°gina {page_num}")
                
                for enlace in enlaces:
                    try:
                        href = await enlace.get_attribute('href')
                        if not href:
                            continue
                        
                        # Construir URL completa
                        full_url = urljoin(url, href)
                        
                        # Filtrar solo URLs de libros (formato: /book/XXXXX.html)
                        if '/book/' in full_url and full_url.endswith('.html'):
                            # Evitar duplicados en esta sesi√≥n
                            if full_url not in [n['url'] for n in novelas_encontradas]:
                                # Intentar extraer el t√≠tulo del enlace
                                titulo = await enlace.inner_text()
                                titulo = titulo.strip() if titulo else "T√≠tulo pendiente"
                                
                                novelas_encontradas.append({
                                    'url': full_url,
                                    'titulo': titulo
                                })
                    except Exception as e:
                        logger.debug(f"Error procesando enlace: {e}")
                        continue
                
                logger.info(f"‚úÖ P√°gina {page_num} procesada. Total acumulado: {len(novelas_encontradas)} novelas")
                
                # Peque√±a pausa entre p√°ginas
                await asyncio.sleep(1)
            
            await browser.close()
            
            # Guardar en base de datos
            logger.info(f"\n{'='*80}")
            logger.info(f"üíæ Guardando novelas en la base de datos...")
            logger.info(f"{'='*80}")
            
            nuevas = 0
            duplicadas = 0
            
            for novela_data in novelas_encontradas:
                url = novela_data['url']
                titulo = novela_data['titulo']
                
                # Verificar si ya existe
                if url in urls_ya_en_db:
                    duplicadas += 1
                    logger.debug(f"‚è≠Ô∏è Ya existe: {titulo}")
                    continue
                
                # Crear nueva novela
                nueva_novela = Novela(
                    titulo_original=titulo,
                    fuente_scraping=url,
                    estado_original='en_progreso',
                    es_verificado=False
                )
                
                db.add(nueva_novela)
                nuevas += 1
                logger.info(f"‚ûï Nueva: {titulo} ‚Üí {url}")
            
            db.commit()
            
            logger.info(f"\n{'='*80}")
            logger.info(f"üìä RESUMEN")
            logger.info(f"{'='*80}")
            logger.info(f"Total encontradas: {len(novelas_encontradas)}")
            logger.info(f"Nuevas guardadas: {nuevas}")
            logger.info(f"Ya exist√≠an: {duplicadas}")
            logger.info(f"{'='*80}")
            
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico: {e}")
            db.rollback()
        finally:
            db.close()

if __name__ == "__main__":
    # Configuraci√≥n
    # Ejemplos de URLs v√°lidas:
    # - https://twkan.com/novels/newhot_0_0_1.html (Nuevas populares)
    # - https://twkan.com/novels/hot (Populares - se convertir√° a hot_0_0_N.html)
    # - https://twkan.com/novels/new (Nuevas - se convertir√° a new_0_0_N.html)
    CATALOG_URL = "https://twkan.com/novels/newhot_0_0_1.html"
    MAX_PAGES = 10  # N√∫mero de p√°ginas a scrapear (ajusta seg√∫n necesites)
    
    logger.info(f"üöÄ Iniciando scraper de cat√°logo de novelas")
    logger.info(f"üìç URL: {CATALOG_URL}")
    logger.info(f"üìÑ P√°ginas a scrapear: {MAX_PAGES}")
    logger.info(f"{'='*80}\n")
    
    asyncio.run(scrape_novel_catalog(CATALOG_URL, MAX_PAGES))
